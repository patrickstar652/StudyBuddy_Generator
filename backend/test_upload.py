"""
æ¸¬è©¦æ–‡ä»¶ä¸Šå‚³å’Œåˆ‡ç‰‡åŠŸèƒ½
é‹è¡Œæ­¤è…³æœ¬ä»¥é©—è­‰ç³»çµ±æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import os

# æ·»åŠ çˆ¶ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from services.document_processor import get_document_processor
from services.rag_service import get_rag_service
import tempfile


def test_document_processing():
    """æ¸¬è©¦æ–‡ä»¶è™•ç†æµç¨‹"""
    
    print("=" * 60)
    print("Study Buddy - æ–‡ä»¶è™•ç†æ¸¬è©¦")
    print("=" * 60)
    print()
    
    # å‰µå»ºæ¸¬è©¦æ–‡ä»¶
    test_content = """
    æ©Ÿå™¨å­¸ç¿’å°è«–
    
    ç¬¬ä¸€ç« ï¼šä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ
    
    æ©Ÿå™¨å­¸ç¿’ï¼ˆMachine Learningï¼‰æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹åˆ†æ”¯ï¼Œå®ƒä½¿é›»è…¦ç³»çµ±èƒ½å¤ å¾æ•¸æ“šä¸­å­¸ç¿’ä¸¦æ”¹é€²ï¼Œ
    è€Œç„¡éœ€æ˜ç¢ºç·¨ç¨‹ã€‚æ©Ÿå™¨å­¸ç¿’ç®—æ³•é€šéåˆ†æå¤§é‡æ•¸æ“šä¾†è­˜åˆ¥æ¨¡å¼ï¼Œä¸¦ä½¿ç”¨é€™äº›æ¨¡å¼ä¾†åšå‡ºé æ¸¬æˆ–æ±ºç­–ã€‚
    
    ä¸»è¦é¡å‹ï¼š
    1. ç›£ç£å¼å­¸ç¿’ï¼ˆSupervised Learningï¼‰
       - ä½¿ç”¨æ¨™è¨˜æ•¸æ“šé€²è¡Œè¨“ç·´
       - ä¾‹å¦‚ï¼šåˆ†é¡ã€è¿´æ­¸
    
    2. éç›£ç£å¼å­¸ç¿’ï¼ˆUnsupervised Learningï¼‰
       - ä½¿ç”¨æœªæ¨™è¨˜æ•¸æ“š
       - ä¾‹å¦‚ï¼šèšé¡ã€é™ç¶­
    
    3. å¼·åŒ–å­¸ç¿’ï¼ˆReinforcement Learningï¼‰
       - é€šéè©¦éŒ¯å­¸ç¿’
       - ä¾‹å¦‚ï¼šéŠæˆ² AIã€æ©Ÿå™¨äººæ§åˆ¶
    
    ç¬¬äºŒç« ï¼šæ·±åº¦å­¸ç¿’
    
    æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹å­é ˜åŸŸï¼Œå®ƒä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²è·¯ä¾†å­¸ç¿’æ•¸æ“šçš„è¤‡é›œè¡¨ç¤ºã€‚
    æ·±åº¦å­¸ç¿’åœ¨åœ–åƒè­˜åˆ¥ã€è‡ªç„¶èªè¨€è™•ç†å’ŒèªéŸ³è­˜åˆ¥ç­‰é ˜åŸŸå–å¾—äº†çªç ´æ€§é€²å±•ã€‚
    
    å¸¸è¦‹æ¶æ§‹ï¼š
    - å·ç©ç¥ç¶“ç¶²è·¯ï¼ˆCNNï¼‰ï¼šç”¨æ–¼åœ–åƒè™•ç†
    - å¾ªç’°ç¥ç¶“ç¶²è·¯ï¼ˆRNNï¼‰ï¼šç”¨æ–¼åºåˆ—æ•¸æ“š
    - Transformerï¼šç”¨æ–¼è‡ªç„¶èªè¨€è™•ç†
    
    æ‡‰ç”¨å¯¦ä¾‹ï¼š
    - è‡ªå‹•é§•é§›æ±½è»Š
    - é†«ç™‚è¨ºæ–·
    - èªéŸ³åŠ©æ‰‹
    - æ¨è–¦ç³»çµ±
    """
    
    # å‰µå»ºè‡¨æ™‚æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
        f.write(test_content)
        temp_file = f.name
    
    try:
        print("ğŸ“„ æ¸¬è©¦æ–‡ä»¶å·²å‰µå»º")
        print(f"   è·¯å¾‘: {temp_file}")
        print(f"   å­—æ•¸: {len(test_content)} å­—å…ƒ")
        print()
        
        # Step 1: æ–‡å­—æå–
        print("Step 1: æå–æ–‡å­—...")
        processor = get_document_processor()
        extracted_text = processor.extract_text(temp_file)
        print(f"   âœ“ æˆåŠŸæå– {len(extracted_text)} å­—å…ƒ")
        print()
        
        # Step 2: ç²å–æ–‡ä»¶è³‡è¨Š
        print("Step 2: åˆ†ææ–‡ä»¶...")
        doc_info = processor.get_document_info(temp_file)
        print(f"   âœ“ ç¸½å­—å…ƒæ•¸: {doc_info['total_characters']:,}")
        print(f"   âœ“ ç¸½ Tokens: {doc_info['total_tokens']:,}")
        print(f"   âœ“ å€å¡Šæ•¸é‡: {doc_info['total_chunks']}")
        print()
        
        # Step 3: åˆ‡ç‰‡è™•ç†
        print("Step 3: æ™ºæ…§åˆ‡ç‰‡...")
        chunks = processor.split_into_chunks(extracted_text)
        print(f"   âœ“ å·²åˆ‡åˆ†ç‚º {len(chunks)} å€‹å€å¡Š")
        print()
        
        # é¡¯ç¤ºå€å¡Šè©³æƒ…
        print("   å€å¡Šè©³æƒ…:")
        for i, chunk in enumerate(chunks[:3]):  # åªé¡¯ç¤ºå‰ 3 å€‹
            preview = chunk['content'][:80].replace('\n', ' ')
            print(f"   å€å¡Š {i+1}:")
            print(f"     - Tokens: {chunk['token_count']}")
            print(f"     - å…§å®¹: {preview}...")
        
        if len(chunks) > 3:
            print(f"   ... é‚„æœ‰ {len(chunks) - 3} å€‹å€å¡Š")
        print()
        
        # Step 4: å‘é‡åµŒå…¥å’Œç´¢å¼•
        print("Step 4: å»ºç«‹å‘é‡ç´¢å¼•...")
        print("   (é€™å¯èƒ½éœ€è¦å¹¾ç§’é˜ï¼Œé¦–æ¬¡æœƒä¸‹è¼‰æ¨¡å‹...)")
        
        rag_service = get_rag_service()
        doc_id = "test-doc-001"
        
        index_result = rag_service.index_document(doc_id, temp_file)
        
        print(f"   âœ“ å·²ç´¢å¼• {index_result['chunks_indexed']} å€‹å€å¡Š")
        print(f"   âœ“ ç¸½ Tokens: {index_result['total_tokens']:,}")
        print(f"   âœ“ åµŒå…¥ç¶­åº¦: 384")
        print()
        
        # Step 5: æ¸¬è©¦æœç´¢
        print("Step 5: æ¸¬è©¦å‘é‡æœç´¢...")
        test_queries = [
            "ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ",
            "æ·±åº¦å­¸ç¿’çš„æ‡‰ç”¨",
            "ç›£ç£å¼å­¸ç¿’çš„ä¾‹å­"
        ]
        
        for query in test_queries:
            results = rag_service.search(doc_id, query, top_k=2)
            print(f"\n   æŸ¥è©¢: '{query}'")
            print(f"   æ‰¾åˆ° {len(results)} å€‹ç›¸é—œå€å¡Š:")
            
            for i, result in enumerate(results):
                preview = result['content'][:100].replace('\n', ' ')
                print(f"     {i+1}. ç›¸ä¼¼åº¦: {result['score']:.3f}")
                print(f"        å…§å®¹: {preview}...")
        
        print()
        print("=" * 60)
        print("âœ… æ‰€æœ‰æ¸¬è©¦é€šéï¼")
        print("=" * 60)
        print()
        print("ç³»çµ±åŠŸèƒ½æ­£å¸¸ï¼Œå¯ä»¥é–‹å§‹ä½¿ç”¨ Study Buddy äº†ï¼")
        print()
        print("ä¸‹ä¸€æ­¥:")
        print("1. å•Ÿå‹•æ‡‰ç”¨: python app.py")
        print("2. ä¸Šå‚³ä½ çš„å­¸ç¿’ææ–™")
        print("3. é–‹å§‹ä½¿ç”¨å­¸ç¿’å·¥å…·")
        print()
        
    except Exception as e:
        print()
        print("=" * 60)
        print("âŒ æ¸¬è©¦å¤±æ•—")
        print("=" * 60)
        print(f"éŒ¯èª¤: {str(e)}")
        print()
        import traceback
        traceback.print_exc()
        
    finally:
        # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
        if os.path.exists(temp_file):
            os.remove(temp_file)
        
        # æ¸…ç† RAG ç´¢å¼•
        try:
            rag_service.remove_document(doc_id)
        except:
            pass


if __name__ == '__main__':
    test_document_processing()
